{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO1EcrPssUGhPZgnObMTqdD",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Ware-Hard-or-Soft/NLP-LLM/blob/main/Deep_RNN_for_sentiment_analysis_ipynb.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Deep RNN for sentiment analysis\n",
        "By David Zheng\n",
        "1. Data Loading and Preprocessing: It loads a dataset of movie reviews, preprocesses them (like converting words to numerical representations), and prepares them for training and testing.\n",
        "2. Network Architecture: It defines an RNN architecture, specifically using GRU (Gated Recurrent Unit) or TEXTnetOrder2 with a hidden state for processing variable-length reviews.\n",
        "3. Training: It trains the network on the labeled movie reviews, adjusting the 4. network's parameters to minimize prediction errors.\n",
        "Testing: It evaluates the trained network's performance on unseen test data, measuring its accuracy in classifying positive and negative reviews."
      ],
      "metadata": {
        "id": "MUnyplyl8gzT"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LjNoCIeRsxAs"
      },
      "source": [
        "#############################################\n",
        "#This code is based on Prof. Kak material\n",
        "#1 using word_index as the integer of the word to be fed in the net,change the dimension of tensors in the network to fit\n",
        "#2 replacing the tanh nonlinear with the sigmoid, adding detach() to cell function, changing learning rate to 1e-7,change the dimension of tensors in the network to fit\n",
        "#3 using batach size =3 and make all the review to the longest,using drop_last to match the batck size, change the dimension of tensors in the network to fit\n",
        "\n",
        "\n",
        "import sys,os,os.path\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchvision\n",
        "import torchvision.transforms as tvt\n",
        "import torch.optim as optim\n",
        "from torchsummary import summary\n",
        "import numpy as np\n",
        "from PIL import ImageFilter\n",
        "import numbers\n",
        "import re\n",
        "import math\n",
        "import random\n",
        "import copy\n",
        "import matplotlib.pyplot as plt\n",
        "import gzip\n",
        "import pickle\n",
        "import time"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZyNF4yxlFOCA",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 173
        },
        "outputId": "3ab67661-a9aa-4cbb-abcd-596461f0314e"
      },
      "source": [
        "!pip3 install pymsgbox\n",
        "import matplotlib\n",
        "matplotlib.use('Agg')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting pymsgbox\n",
            "  Downloading https://files.pythonhosted.org/packages/ac/e0/0ac1ac67178a71b92e46f46788ddd799bb40bff40acd60c47c50be170374/PyMsgBox-1.0.7.tar.gz\n",
            "Building wheels for collected packages: pymsgbox\n",
            "  Building wheel for pymsgbox (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pymsgbox: filename=PyMsgBox-1.0.7-cp36-none-any.whl size=7321 sha256=8fe840b5073db78869cc84a846dd6eaabe9e9c52bb21b7c388de9175acef9fc0\n",
            "  Stored in directory: /root/.cache/pip/wheels/8e/62/9f/951a04461ec012e443f9aa172598fc8f9c6e409bf753687fad\n",
            "Successfully built pymsgbox\n",
            "Installing collected packages: pymsgbox\n",
            "Successfully installed pymsgbox-1.0.7\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QyYqRifFtoAZ",
        "outputId": "5a7f4cc9-5bb3-46d2-9c3d-621c5d5df1f8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 124
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "dataroot = '/content/gdrive/My Drive/ECE695/DLStudio/DLStudio-1.1.0/Examples/data/'"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hVWyS-5wD7DN"
      },
      "source": [
        "class DLStudio(object):\n",
        "\n",
        "    def __init__(self, *args, **kwargs ):\n",
        "        if args:\n",
        "            raise ValueError(\n",
        "                   '''DLStudio constructor can only be called with keyword arguments for\n",
        "                      the following keywords: epochs, learning_rate, batch_size, momentum,\n",
        "                      convo_layers_config, image_size, dataroot, path_saved_model, classes,\n",
        "                      image_size, convo_layers_config, fc_layers_config, debug_train, use_gpu, and\n",
        "                      debug_test''')\n",
        "        learning_rate = epochs = batch_size = convo_layers_config = momentum = None\n",
        "        image_size = fc_layers_config = dataroot =  path_saved_model = classes = use_gpu = None\n",
        "        debug_train  = debug_test = None\n",
        "        if 'dataroot' in kwargs                      :   dataroot = kwargs.pop('dataroot')\n",
        "        if 'learning_rate' in kwargs                 :   learning_rate = kwargs.pop('learning_rate')\n",
        "        if 'momentum' in kwargs                      :   momentum = kwargs.pop('momentum')\n",
        "        if 'epochs' in kwargs                        :   epochs = kwargs.pop('epochs')\n",
        "        if 'batch_size' in kwargs                    :   batch_size = kwargs.pop('batch_size')\n",
        "        if 'convo_layers_config' in kwargs           :   convo_layers_config = kwargs.pop('convo_layers_config')\n",
        "        if 'image_size' in kwargs                    :   image_size = kwargs.pop('image_size')\n",
        "        if 'fc_layers_config' in kwargs              :   fc_layers_config = kwargs.pop('fc_layers_config')\n",
        "        if 'path_saved_model' in kwargs              :   path_saved_model = kwargs.pop('path_saved_model')\n",
        "        if 'classes' in kwargs                       :   classes = kwargs.pop('classes')\n",
        "        if 'use_gpu' in kwargs                       :   use_gpu = kwargs.pop('use_gpu')\n",
        "        if 'debug_train' in kwargs                   :   debug_train = kwargs.pop('debug_train')\n",
        "        if 'debug_test' in kwargs                    :   debug_test = kwargs.pop('debug_test')\n",
        "        if len(kwargs) != 0: raise ValueError('''You have provided unrecognizable keyword args''')\n",
        "        if dataroot:\n",
        "            self.dataroot = dataroot\n",
        "        if convo_layers_config:\n",
        "            self.convo_layers_config = convo_layers_config\n",
        "        if image_size:\n",
        "            self.image_size = image_size\n",
        "        if fc_layers_config:\n",
        "            self.fc_layers_config = fc_layers_config\n",
        "            if fc_layers_config[0] is not -1:\n",
        "                raise Exception(\"\"\"\\n\\n\\nYour 'fc_layers_config' construction option is not correct. \"\"\"\n",
        "                                \"\"\"The first element of the list of nodes in the fc layer must be -1 \"\"\"\n",
        "                                \"\"\"because the input to fc will be set automatically to the size of \"\"\"\n",
        "                                \"\"\"the final activation volume of the convolutional part of the network\"\"\")\n",
        "        if  path_saved_model:\n",
        "            self.path_saved_model = path_saved_model\n",
        "        if classes:\n",
        "            self.class_labels = classes\n",
        "        if learning_rate:\n",
        "            self.learning_rate = learning_rate\n",
        "        else:\n",
        "            self.learning_rate = 1e-6\n",
        "        if momentum:\n",
        "            self.momentum = momentum\n",
        "        if epochs:\n",
        "            self.epochs = epochs\n",
        "        if batch_size:\n",
        "            self.batch_size = batch_size\n",
        "        if use_gpu is not None:\n",
        "            self.use_gpu = use_gpu\n",
        "            if use_gpu is True:\n",
        "                if torch.cuda.is_available():\n",
        "                    self.device = torch.device(\"cuda:0\")\n",
        "                else:\n",
        "                    raise Exception(\"You requested GPU support, but there's no GPU on this machine\")\n",
        "            #else:\n",
        "                #self.device = torch.device(\"cpu\")\n",
        "        if debug_train:\n",
        "            self.debug_train = debug_train\n",
        "        else:\n",
        "            self.debug_train = 0\n",
        "        if debug_test:\n",
        "            self.debug_test = debug_test\n",
        "        else:\n",
        "            self.debug_test = 0\n",
        "        self.debug_config = 0\n",
        "#        self.device = torch.device(\"cuda:0\" if torch.cuda.is_available() and self.use_gpu is False else \"cpu\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ymMy6bs_DMG4"
      },
      "source": [
        "    device = torch.device(\"cuda:0\")\n",
        "    class TextClassification(nn.Module):\n",
        "        \"\"\"\n",
        "        The purpose of this inner class is to be able to use the DLStudio module for simple\n",
        "        experiments in text classification.  Consider, for example, the problem of automatic\n",
        "        classification of variable-length user feedback: you want to create a neural network\n",
        "        that can label an uploaded product review of arbitrary length as positive or negative.\n",
        "        One way to solve this problem is with a recurrent neural network in which you use a\n",
        "        hidden state for characterizing a variable-length product review with a fixed-length\n",
        "        state vector.  This inner class allows you to carry out such experiments.\n",
        "        \"\"\"\n",
        "        def __init__(self, dl_studio, dataserver_train=None, dataserver_test=None, dataset_file_train=None, dataset_file_test=None):\n",
        "            super(TextClassification, self).__init__()\n",
        "            self.dl_studio = dl_studio\n",
        "            self.dataserver_train = dataserver_train\n",
        "            self.dataserver_test = dataserver_test\n",
        "\n",
        "        class SentimentAnalysisDataset(torch.utils.data.Dataset):\n",
        "\n",
        "            def __init__(self, dl_studio, train_or_test, dataset_file):\n",
        "                super(TextClassification.SentimentAnalysisDataset, self).__init__()\n",
        "                self.train_or_test = train_or_test\n",
        "                root_dir = dl_studio.dataroot\n",
        "                f = gzip.open(root_dir + dataset_file, 'rb')\n",
        "                dataset = f.read()\n",
        "                if train_or_test is 'train':\n",
        "                    if sys.version_info[0] == 3:\n",
        "                        self.positive_reviews_train, self.negative_reviews_train, self.vocab = pickle.loads(dataset, encoding='latin1')\n",
        "                    else:\n",
        "                        self.positive_reviews_train, self.negative_reviews_train, self.vocab = pickle.loads(dataset)\n",
        "                    self.categories = sorted(list(self.positive_reviews_train.keys()))\n",
        "                    self.category_sizes_train_pos = {category : len(self.positive_reviews_train[category]) for category in self.categories}\n",
        "                    self.category_sizes_train_neg = {category : len(self.negative_reviews_train[category]) for category in self.categories}\n",
        "                    self.indexed_dataset_train = []\n",
        "                    self.train_review_len = []\n",
        "                    self.test_review_len = []\n",
        "                    for category in self.positive_reviews_train:\n",
        "                        for review in self.positive_reviews_train[category]:\n",
        "                            self.indexed_dataset_train.append([review, category, 1])\n",
        "                            self.train_review_len.append(len(review))\n",
        "                    for category in self.negative_reviews_train:\n",
        "                        for review in self.negative_reviews_train[category]:\n",
        "                            self.indexed_dataset_train.append([review, category, 0])\n",
        "                            self.train_review_len.append(len(review))\n",
        "                    random.shuffle(self.indexed_dataset_train)\n",
        "                    self.max_review_len = max(self.train_review_len)\n",
        "                elif train_or_test is 'test':\n",
        "                    if sys.version_info[0] == 3:\n",
        "                        self.positive_reviews_test, self.negative_reviews_test, self.vocab = pickle.loads(dataset, encoding='latin1')\n",
        "                    else:\n",
        "                        self.positive_reviews_test, self.negative_reviews_test, self.vocab = pickle.loads(dataset)\n",
        "                    self.vocab = sorted(self.vocab)\n",
        "                    self.categories = sorted(list(self.positive_reviews_test.keys()))\n",
        "                    self.category_sizes_test_pos = {category : len(self.positive_reviews_test[category]) for category in self.categories}\n",
        "                    self.category_sizes_test_neg = {category : len(self.negative_reviews_test[category]) for category in self.categories}\n",
        "                    self.indexed_dataset_test = []\n",
        "                    self.train_review_len = []\n",
        "                    self.test_review_len = []\n",
        "                    for category in self.positive_reviews_test:\n",
        "                        for review in self.positive_reviews_test[category]:\n",
        "                            self.indexed_dataset_test.append([review, category, 1])\n",
        "                            self.test_review_len.append(len(review))\n",
        "                    for category in self.negative_reviews_test:\n",
        "                        for review in self.negative_reviews_test[category]:\n",
        "                            self.indexed_dataset_test.append([review, category, 0])\n",
        "                            self.test_review_len.append(len(review))\n",
        "                    random.shuffle(self.indexed_dataset_test)\n",
        "                    self.max_review_len = max(self.test_review_len)\n",
        "                #print(max_review_len)\n",
        "            def get_vocab_size(self):\n",
        "                return len(self.vocab)\n",
        "\n",
        "            #def one_hotvec_for_word(self, word):\n",
        "                #word_index =  self.vocab.index(word)\n",
        "                #hotvec = torch.zeros(1, len(self.vocab))\n",
        "                #hotvec[0, word_index] = 1\n",
        "\n",
        "                #hotvec=torch.as_tensor(word_index)\n",
        "                #print(hotvec)\n",
        "                #return word_index\n",
        "##################################################################################\n",
        "            def review_to_tensor(self, review):\n",
        "                #wordlist=[]\n",
        "                review_tensor = torch.zeros(len(review),1)\n",
        "                for i,word in enumerate(review):\n",
        "                    word_index =  self.vocab.index(word)\n",
        "                    #m = one_hotvec_for_word(word)\n",
        "                    #wordlist.append(word_index)\n",
        "                    review_tensor[i,:]=torch.as_tensor(word_index)\n",
        "                return review_tensor\n",
        "##############################################################################\n",
        "            def sentiment_to_tensor(self, sentiment):\n",
        "                sentiment_tensor = torch.zeros(2)\n",
        "                if sentiment is 1:\n",
        "                    sentiment_tensor[1] = 1\n",
        "                elif sentiment is 0:\n",
        "                    sentiment_tensor[0] = 1\n",
        "                sentiment_tensor = sentiment_tensor.type(torch.long)\n",
        "                return sentiment_tensor\n",
        "\n",
        "            def __len__(self):\n",
        "                if self.train_or_test is 'train':\n",
        "                    return len(self.indexed_dataset_train)\n",
        "                elif self.train_or_test is 'test':\n",
        "                    return len(self.indexed_dataset_test)\n",
        "\n",
        "            def __getitem__(self, idx):\n",
        "                sample = self.indexed_dataset_train[idx] if self.train_or_test is 'train' else self.indexed_dataset_test[idx]\n",
        "                review = sample[0]\n",
        "                review_category = sample[1]\n",
        "                review_sentiment = sample[2]\n",
        "                review_sentiment = self.sentiment_to_tensor(review_sentiment)\n",
        "                review_tensor = self.review_to_tensor(review)\n",
        "                category_index = self.categories.index(review_category)\n",
        "                sample = {'review'       : review_tensor,\n",
        "                          'category'     : category_index, # should be converted to tensor, but not yet used\n",
        "                          'sentiment'    : review_sentiment }\n",
        "                return sample\n",
        "\n",
        "        def load_SentimentAnalysisDataset(self, dataserver_train, dataserver_test ):\n",
        "            self.train_dataloader = torch.utils.data.DataLoader(dataserver_train,\n",
        "                        batch_size=self.dl_studio.batch_size,shuffle=True, num_workers=1,drop_last=True)\n",
        "            self.test_dataloader = torch.utils.data.DataLoader(dataserver_test,\n",
        "                               batch_size=self.dl_studio.batch_size,shuffle=False, num_workers=1,drop_last=True)\n",
        "\n",
        "        class TEXTnet(nn.Module):\n",
        "            \"\"\"\n",
        "            TEXTnet stands for \"Text Classification Network\".\n",
        "            This network is meant for semantic classification of variable length sentiment\n",
        "            data.  Based on my limited testing, the performance of this network is rather\n",
        "            poor because it has no protection against vanishing gradients when used in an\n",
        "            RNN.\n",
        "            Location: Inner class TextClassification\n",
        "            \"\"\"\n",
        "            def __init__(self, input_size, hidden_size, output_size):\n",
        "                super(TextClassification.TEXTnet, self).__init__()\n",
        "                self.input_size = input_size\n",
        "                self.hidden_size = hidden_size\n",
        "                self.output_size = output_size\n",
        "                self.combined_to_hidden = nn.Linear(input_size + hidden_size, hidden_size)\n",
        "                self.combined_to_middle = nn.Linear(input_size + hidden_size, 100)\n",
        "                self.middle_to_out = nn.Linear(100, output_size)\n",
        "                self.logsoftmax = nn.LogSoftmax(dim=1)\n",
        "                self.dropout = nn.Dropout(p=0.1)\n",
        "\n",
        "            def forward(self, input, hidden):\n",
        "                combined = torch.cat((input, hidden), 1)\n",
        "                hidden = self.combined_to_hidden(combined)\n",
        "                out = self.combined_to_middle(combined)\n",
        "                out = torch.nn.functional.relu(out)\n",
        "                out = self.dropout(out)\n",
        "                out = self.middle_to_out(out)\n",
        "                out = self.logsoftmax(out)\n",
        "                return out,hidden\n",
        "\n",
        "        class TEXTnetOrder2(nn.Module):\n",
        "\n",
        "            def __init__(self, input_size, hidden_size, output_size, dls):\n",
        "                super(TextClassification.TEXTnetOrder2, self).__init__()\n",
        "                self.input_size = input_size\n",
        "                self.hidden_size = hidden_size\n",
        "                self.output_size = output_size\n",
        "                self.combined_to_hidden = nn.Linear(input_size + 2*hidden_size, hidden_size)\n",
        "                self.combined_to_middle = nn.Linear(input_size + 2*hidden_size, 100)\n",
        "                self.middle_to_out = nn.Linear(100, output_size)\n",
        "                self.logsoftmax = nn.LogSoftmax(dim=1)\n",
        "                self.dropout = nn.Dropout(p=0.1)\n",
        "                # for the cell\n",
        "                self.cell = torch.zeros(1, hidden_size).detach().to(device)\n",
        "                self.linear_for_cell = nn.Linear(hidden_size, hidden_size).to(device)\n",
        "\n",
        "            def forward(self, input, hidden):\n",
        "                combined = torch.cat((input, hidden, self.cell), 1)\n",
        "                hidden = self.combined_to_hidden(combined)\n",
        "                out = self.combined_to_middle(combined)\n",
        "                out = torch.nn.functional.relu(out)\n",
        "                out = self.dropout(out)\n",
        "                out = self.middle_to_out(out)\n",
        "                out = self.logsoftmax(out)\n",
        "                hidden_clone = hidden.clone()\n",
        "                self.cell = torch.sigmoid(self.linear_for_cell(hidden_clone)).detach()\n",
        "                return out,hidden\n",
        "\n",
        "        class GRUnet(nn.Module):\n",
        "\n",
        "            def __init__(self, input_size, hidden_size, output_size, n_layers, drop_prob=0.2):\n",
        "                super(TextClassification.GRUnet, self).__init__()\n",
        "                self.hidden_size = hidden_size\n",
        "                self.n_layers = n_layers\n",
        "                self.gru = nn.GRU(input_size, hidden_size, n_layers, batch_first=True, dropout=drop_prob)\n",
        "                self.fc = nn.Linear(hidden_size, output_size)\n",
        "                self.relu = nn.ReLU()\n",
        "                self.logsoftmax = nn.LogSoftmax(dim=1)\n",
        "\n",
        "            def forward(self, x, h):\n",
        "                out, h = self.gru(x, h)\n",
        "                out = self.fc(self.relu(out[:,-1]))\n",
        "                out = self.logsoftmax(out)\n",
        "                return out, h\n",
        "\n",
        "            def init_hidden(self, batch_size):\n",
        "                weight = next(self.parameters()).data\n",
        "                hidden = weight.new(self.n_layers, batch_size, self.hidden_size).zero_()\n",
        "                return hidden\n",
        "\n",
        "        def save_model(self, model):\n",
        "            \"Save the trained model to a disk file\"\n",
        "            torch.save(model.state_dict(), self.dl_studio.path_saved_model)\n",
        "\n",
        "        def run_code_for_training_for_text_classification_no_gru(self, net, hidden_size):\n",
        "            filename_for_out = \"performance_numbers_\" + str(self.dl_studio.epochs) + \".txt\"\n",
        "            FILE = open(filename_for_out, 'w')\n",
        "            net = copy.deepcopy(net)\n",
        "            net = net.to(self.dl_studio.device)\n",
        "            ## Note that the TEXTnet and TEXTnetOrder2 both produce LogSoftmax output:\n",
        "            criterion = nn.NLLLoss()\n",
        "#            criterion = nn.MSELoss()\n",
        "#            criterion = nn.CrossEntropyLoss()\n",
        "            accum_times = []\n",
        "            optimizer = optim.SGD(net.parameters(),\n",
        "                         lr=self.dl_studio.learning_rate, momentum=self.dl_studio.momentum)\n",
        "            start_time = time.clock()\n",
        "            for epoch in range(self.dl_studio.epochs):\n",
        "                print(\"\")\n",
        "                running_loss = 0.0\n",
        "                for i, data in enumerate(self.train_dataloader):\n",
        "                    hidden = torch.zeros(1, hidden_size).to(self.dl_studio.device)\n",
        "                    hidden = hidden.to(self.dl_studio.device)\n",
        "                    review_tensor,category,sentiment = data['review'], data['category'], data['sentiment']\n",
        "                    review_tensor = review_tensor.to(self.dl_studio.device)\n",
        "                    sentiment = sentiment.to(self.dl_studio.device)\n",
        "                    optimizer.zero_grad()\n",
        "                    input = torch.zeros(1,review_tensor.shape[2]).to(self.dl_studio.device)\n",
        "                    input = input.to(self.dl_studio.device)\n",
        "                    for k in range(review_tensor.shape[1]):\n",
        "                        input[0,:] = review_tensor[0,k]#.to(self.dl_studio.device)\n",
        "                        output, hidden = net(input, hidden)\n",
        "                    loss = criterion(output, torch.argmax(sentiment,1))\n",
        "                    running_loss += loss.item()\n",
        "                    loss.backward(retain_graph=True)\n",
        "                    torch.autograd.set_detect_anomaly(True)\n",
        "                    optimizer.step()\n",
        "                    if i % 100==99:\n",
        "                        avg_loss = running_loss / float(100)\n",
        "                        current_time = time.clock()\n",
        "                        time_elapsed = current_time-start_time\n",
        "                        print(\"[epoch:%d  iter:%4d  elapsed_time: %4d secs]     loss: %.3f\" % (epoch+1,i+1, time_elapsed,avg_loss))\n",
        "                        accum_times.append(current_time-start_time)\n",
        "                        FILE.write(\"%.3f\\n\" % avg_loss)\n",
        "                        FILE.flush()\n",
        "                        running_loss = 0.0\n",
        "            print(\"\\nFinished Training\\n\")\n",
        "            self.save_model(net)\n",
        "\n",
        "        def run_code_for_training_for_text_classification_with_gru(self, net, hidden_size):\n",
        "            filename_for_out = \"performance_numbers_\" + str(self.dl_studio.epochs) + \".txt\"\n",
        "            FILE = open(filename_for_out, 'w')\n",
        "            net = copy.deepcopy(net)\n",
        "            net = net.to(self.dl_studio.device)\n",
        "            ##  Note that the GREnet now produces the LogSoftmax output:\n",
        "            criterion = nn.NLLLoss()\n",
        "#            criterion = nn.MSELoss()\n",
        "#            criterion = nn.CrossEntropyLoss()\n",
        "            accum_times = []\n",
        "            optimizer = optim.SGD(net.parameters(),\n",
        "                         lr=self.dl_studio.learning_rate, momentum=self.dl_studio.momentum)\n",
        "            for epoch in range(self.dl_studio.epochs):\n",
        "                print(\"\")\n",
        "                running_loss = 0.0\n",
        "                start_time = time.clock()\n",
        "                for i, data in enumerate(self.train_dataloader):\n",
        "                    review_tensor,category,sentiment = data['review'], data['category'], data['sentiment']\n",
        "                    review_tensor = review_tensor.to(self.dl_studio.device)\n",
        "                    sentiment = sentiment.to(self.dl_studio.device)\n",
        "                    ## The following type conversion needed for MSELoss:\n",
        "                    ##sentiment = sentiment.float()\n",
        "                    optimizer.zero_grad()\n",
        "                    hidden = net.init_hidden(3).to(self.dl_studio.device)\n",
        "                    for k in range(review_tensor.shape[0]):\n",
        "                        output, hidden = net(torch.unsqueeze(review_tensor[:,k],1), hidden)\n",
        "                    ## If using NLLLoss, CrossEntropyLoss\n",
        "                    loss = criterion(output, torch.argmax(sentiment, 1))\n",
        "                    ## If using MSELoss:\n",
        "                    ## loss = criterion(output, sentiment)\n",
        "                    running_loss += loss.item()\n",
        "                    loss.backward()\n",
        "                    optimizer.step()\n",
        "                    if i % 100 == 99:\n",
        "                        avg_loss = running_loss / float(100)\n",
        "                        current_time = time.clock()\n",
        "                        time_elapsed = current_time-start_time\n",
        "                        print(\"[epoch:%d  iter:%4d  elapsed_time:%4d secs]     loss: %.3f\" % (epoch+1,i+1, time_elapsed,avg_loss))\n",
        "                        accum_times.append(current_time-start_time)\n",
        "                        FILE.write(\"%.3f\\n\" % avg_loss)\n",
        "                        FILE.flush()\n",
        "                        running_loss = 0.0\n",
        "            print(\"Total Training Time: {}\".format(str(sum(accum_times))))\n",
        "            print(\"\\nFinished Training\\n\")\n",
        "            self.save_model(net)\n",
        "\n",
        "\n",
        "        def run_code_for_testing_text_classification_with_gru_0(self, net, hidden_size):\n",
        "            net.load_state_dict(torch.load(self.dl_studio.path_saved_model))\n",
        "            classification_accuracy = 0.0\n",
        "            negative_total = 0\n",
        "            positive_total = 0\n",
        "            confusion_matrix = torch.zeros(2,2)\n",
        "            with torch.no_grad():\n",
        "                for i, data in enumerate(self.test_dataloader):\n",
        "                    review_tensor,category,sentiment = data['review'], data['category'], data['sentiment']\n",
        "                    hidden = net.init_hidden(1)\n",
        "                    for k in range(review_tensor.shape[1]):\n",
        "                        output, hidden = net(torch.unsqueeze(torch.unsqueeze(review_tensor[0,k],0),0), hidden)\n",
        "                    predicted_idx = torch.argmax(output).item()\n",
        "                    gt_idx = torch.argmax(sentiment).item()\n",
        "                    if i % 100 == 99:\n",
        "                        print(\"   [i=%d]    predicted_label=%d       gt_label=%d\\n\\n\" % (i+1, predicted_idx,gt_idx))\n",
        "                    if predicted_idx == gt_idx:\n",
        "                        classification_accuracy += 1\n",
        "                    if gt_idx is 0:\n",
        "                        negative_total += 1\n",
        "                    elif gt_idx is 1:\n",
        "                        positive_total += 1\n",
        "                    confusion_matrix[gt_idx,predicted_idx] += 1\n",
        "            out_percent = np.zeros((2,2), dtype='float')\n",
        "            print(\"\\n\\nNumber of positive reviews tested: %d\" % positive_total)\n",
        "            print(\"\\n\\nNumber of negative reviews tested: %d\" % negative_total)\n",
        "            print(\"\\n\\nDisplaying the confusion matrix:\\n\")\n",
        "            out_str = \"                      \"\n",
        "            out_str +=  \"%18s    %18s\" % ('predicted negative', 'predicted positive')\n",
        "            print(out_str + \"\\n\")\n",
        "            for i,label in enumerate(['true negative', 'true positive']):\n",
        "                out_percent[0,0] = \"%.3f\" % (100 * confusion_matrix[0,0] / float(negative_total))\n",
        "                out_percent[0,1] = \"%.3f\" % (100 * confusion_matrix[0,1] / float(negative_total))\n",
        "                out_percent[1,0] = \"%.3f\" % (100 * confusion_matrix[1,0] / float(positive_total))\n",
        "                out_percent[1,1] = \"%.3f\" % (100 * confusion_matrix[1,1] / float(positive_total))\n",
        "                out_str = \"%12s:  \" % label\n",
        "                for j in range(2):\n",
        "                    out_str +=  \"%18s\" % out_percent[i,j]\n",
        "                print(out_str)\n",
        "\n",
        "        def run_code_for_training_for_text_classification_with_gru_0(self, net, hidden_size):\n",
        "            filename_for_out = \"performance_numbers_\" + str(self.dl_studio.epochs) + \".txt\"\n",
        "            FILE = open(filename_for_out, 'w')\n",
        "            net = copy.deepcopy(net)\n",
        "            net = net.to(self.dl_studio.device)\n",
        "            ##  Note that the GREnet now produces the LogSoftmax output:\n",
        "            criterion = nn.NLLLoss()\n",
        "#            criterion = nn.MSELoss()\n",
        "#            criterion = nn.CrossEntropyLoss()\n",
        "            accum_times = []\n",
        "            optimizer = optim.SGD(net.parameters(),\n",
        "                         lr=self.dl_studio.learning_rate, momentum=self.dl_studio.momentum)\n",
        "            for epoch in range(self.dl_studio.epochs):\n",
        "                print(\"\")\n",
        "                running_loss = 0.0\n",
        "                start_time = time.clock()\n",
        "                for i, data in enumerate(self.train_dataloader):\n",
        "                    review_tensor,category,sentiment = data['review'], data['category'], data['sentiment']\n",
        "                    review_tensor = review_tensor.to(self.dl_studio.device)\n",
        "                    sentiment = sentiment.to(self.dl_studio.device)\n",
        "                    ## The following type conversion needed for MSELoss:\n",
        "                    ##sentiment = sentiment.float()\n",
        "                    optimizer.zero_grad()\n",
        "                    hidden = net.init_hidden(1).to(self.dl_studio.device)\n",
        "                    for k in range(review_tensor.shape[1]):\n",
        "                        output, hidden = net(torch.unsqueeze(torch.unsqueeze(review_tensor[0,k],0),0), hidden)\n",
        "                    ## If using NLLLoss, CrossEntropyLoss\n",
        "                    loss = criterion(output, torch.argmax(sentiment, 1))\n",
        "                    ## If using MSELoss:\n",
        "                    ## loss = criterion(output, sentiment)\n",
        "                    running_loss += loss.item()\n",
        "                    loss.backward()\n",
        "                    optimizer.step()\n",
        "                    if i % 100 == 99:\n",
        "                        avg_loss = running_loss / float(100)\n",
        "                        current_time = time.clock()\n",
        "                        time_elapsed = current_time-start_time\n",
        "                        print(\"[epoch:%d  iter:%4d  elapsed_time:%4d secs]     loss: %.3f\" % (epoch+1,i+1, time_elapsed,avg_loss))\n",
        "                        accum_times.append(current_time-start_time)\n",
        "                        FILE.write(\"%.3f\\n\" % avg_loss)\n",
        "                        FILE.flush()\n",
        "                        running_loss = 0.0\n",
        "            print(\"Total Training Time: {}\".format(str(sum(accum_times))))\n",
        "            print(\"\\nFinished Training\\n\")\n",
        "            self.save_model(net)\n",
        "\n",
        "        def run_code_for_testing_text_classification_with_gru(self, net, hidden_size):\n",
        "                    net.load_state_dict(torch.load(self.dl_studio.path_saved_model))\n",
        "                    classification_accuracy = 0.0\n",
        "                    negative_total = 0\n",
        "                    positive_total = 0\n",
        "                    predicted_idx=[]\n",
        "                    gt_idx=[]\n",
        "                    #outputlist=[]\n",
        "                    #senlist=[]\n",
        "                    confusion_matrix = torch.zeros(2,2)\n",
        "                    with torch.no_grad():\n",
        "                        for i, data in enumerate(self.test_dataloader):\n",
        "                            review_tensor,category,sentiment = data['review'], data['category'], data['sentiment']\n",
        "                            hidden = net.init_hidden(3)\n",
        "\n",
        "                            for k in range(review_tensor.shape[0]):\n",
        "                                output, hidden = net(torch.unsqueeze(review_tensor[:,k],1), hidden)\n",
        "                                #print(output.shape)\n",
        "                                predicted_idx=torch.argmax(output,dim=1).tolist()\n",
        "                                #print(outputlist)\n",
        "                                gt_idx=torch.argmax(sentiment,dim=1).tolist()\n",
        "                                for m in range(self.dl_studio.batch_size):\n",
        "\n",
        "                                    #if i % 100 == 99:\n",
        "                                        #print(\"   [i=%d]    predicted_label=%d       gt_label=%d\\n\\n\" % (i+1, predicted_idx,gt_idx))\n",
        "                                    if predicted_idx[m] == gt_idx[m]:\n",
        "                                        classification_accuracy += 1\n",
        "                                    if gt_idx[m] is 0:\n",
        "                                        negative_total += 1\n",
        "                                    elif gt_idx[m] is 1:\n",
        "                                        positive_total += 1\n",
        "                                    confusion_matrix[gt_idx[m],predicted_idx[m]] += 1\n",
        "                    out_percent = np.zeros((2,2), dtype='float')\n",
        "                    print(\"\\n\\nNumber of positive reviews tested: %d\" % positive_total)\n",
        "                    print(\"\\n\\nNumber of negative reviews tested: %d\" % negative_total)\n",
        "                    print(\"\\n\\nDisplaying the confusion matrix:\\n\")\n",
        "                    out_str = \"                      \"\n",
        "                    out_str +=  \"%18s    %18s\" % ('predicted negative', 'predicted positive')\n",
        "                    print(out_str + \"\\n\")\n",
        "                    for i,label in enumerate(['true negative', 'true positive']):\n",
        "                        out_percent[0,0] = \"%.3f\" % (100 * confusion_matrix[0,0] / float(negative_total))\n",
        "                        out_percent[0,1] = \"%.3f\" % (100 * confusion_matrix[0,1] / float(negative_total))\n",
        "                        out_percent[1,0] = \"%.3f\" % (100 * confusion_matrix[1,0] / float(positive_total))\n",
        "                        out_percent[1,1] = \"%.3f\" % (100 * confusion_matrix[1,1] / float(positive_total))\n",
        "                        out_str = \"%12s:  \" % label\n",
        "                        for j in range(2):\n",
        "                            out_str +=  \"%18s\" % out_percent[i,j]\n",
        "                        print(out_str)\n",
        "\n",
        "        def run_code_for_testing_text_classification_no_gru(self, net, hidden_size):\n",
        "            net.load_state_dict(torch.load(self.dl_studio.path_saved_model))\n",
        "            classification_accuracy = 0.0\n",
        "            negative_total = 0\n",
        "            positive_total = 0\n",
        "            confusion_matrix = torch.zeros(2,2)\n",
        "            with torch.no_grad():\n",
        "                for i, data in enumerate(self.test_dataloader):\n",
        "                    review_tensor,category,sentiment = data['review'], data['category'], data['sentiment']\n",
        "                    input = torch.zeros(1,review_tensor.shape[2]).to(self.dl_studio.device)\n",
        "                    hidden = torch.zeros(1, hidden_size).to(self.dl_studio.device)\n",
        "                    for k in range(review_tensor.shape[1]):\n",
        "                        input[0,:] = review_tensor[0,k]\n",
        "                        output, hidden = net(input, hidden).to(self.dl_studio.device)\n",
        "                    predicted_idx = torch.argmax(output).item()\n",
        "                    gt_idx = torch.argmax(sentiment).item()\n",
        "                    if i % 100 == 99:\n",
        "                        print(\"   [i=%4d]    predicted_label=%d       gt_label=%d\" % (i+1, predicted_idx,gt_idx))\n",
        "                    if predicted_idx == gt_idx:\n",
        "                        classification_accuracy += 1\n",
        "                    if gt_idx is 0:\n",
        "                        negative_total += 1\n",
        "                    elif gt_idx is 1:\n",
        "                        positive_total += 1\n",
        "                    confusion_matrix[gt_idx,predicted_idx] += 1\n",
        "            out_percent = np.zeros((2,2), dtype='float')\n",
        "            print(\"\\n\\nNumber of positive reviews tested: %d\" % positive_total)\n",
        "            print(\"\\n\\nNumber of negative reviews tested: %d\" % negative_total)\n",
        "            print(\"\\n\\nDisplaying the confusion matrix:\\n\")\n",
        "            out_str = \"                      \"\n",
        "            out_str +=  \"%18s    %18s\" % ('predicted negative', 'predicted positive')\n",
        "            print(out_str + \"\\n\")\n",
        "            for i,label in enumerate(['true negative', 'true positive']):\n",
        "                out_percent[0,0] = \"%.3f\" % (100 * confusion_matrix[0,0] / float(negative_total))\n",
        "                out_percent[0,1] = \"%.3f\" % (100 * confusion_matrix[0,1] / float(negative_total))\n",
        "                out_percent[1,0] = \"%.3f\" % (100 * confusion_matrix[1,0] / float(positive_total))\n",
        "                out_percent[1,1] = \"%.3f\" % (100 * confusion_matrix[1,1] / float(positive_total))\n",
        "                out_str = \"%12s:  \" % label\n",
        "                for j in range(2):\n",
        "                    out_str +=  \"%18s\" % out_percent[i,j]\n",
        "                print(out_str)\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BWFFCEDvcLqm",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "86e563da-3b61-41ef-d4f6-517b85abd55f"
      },
      "source": [
        "#!/usr/bin/env python\n",
        "\n",
        "##  text_classification_with_gru.py\n",
        "\n",
        "\"\"\"\n",
        "This script is an attempt at solving the sentiment classification problem\n",
        "with an RNN that uses a GRU to get around the problem of vanishing gradients\n",
        "that are common to neural networks with feedback.\n",
        "\"\"\"\n",
        "\n",
        "import random\n",
        "import numpy\n",
        "import torch\n",
        "import os, sys\n",
        "\n",
        "\n",
        "seed = 0\n",
        "random.seed(seed)\n",
        "torch.manual_seed(seed)\n",
        "torch.cuda.manual_seed(seed)\n",
        "numpy.random.seed(seed)\n",
        "torch.backends.cudnn.deterministic=True\n",
        "torch.backends.cudnn.benchmarks=False\n",
        "os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "\n",
        "\n",
        "##  watch -d -n 0.5 nvidia-smi\n",
        "\n",
        "\n",
        "dls = DLStudio(\n",
        "#                  dataroot = \"/home/kak/TextDatasets/\",\n",
        "                  dataroot = '/content/gdrive/My Drive/ECE695/DLStudio/DLStudio-1.1.0/Examples/data/',\n",
        "                  path_saved_model = \"./saved_model\",\n",
        "                  momentum = 0.9,\n",
        "#                  learning_rate =  0.004,\n",
        "                  learning_rate =  1e-7,\n",
        "                  epochs = 1,\n",
        "                  batch_size = 1,\n",
        "                  classes = ('negative','positive'),\n",
        "                  debug_train = 1,\n",
        "                  debug_test = 1,\n",
        "                  use_gpu = True,\n",
        "              )\n",
        "\n",
        "\n",
        "text_cl = TextClassification( dl_studio = dls )\n",
        "dataserver_train = TextClassification.SentimentAnalysisDataset(\n",
        "                                 train_or_test = 'train',\n",
        "                                 dl_studio = dls,\n",
        " #                               dataset_file = \"sentiment_dataset_train_3.tar.gz\",\n",
        "                               dataset_file = \"sentiment_dataset_train_200.tar.gz\",\n",
        "#                                dataset_file = \"sentiment_dataset_train_40.tar.gz\",\n",
        "                                                                      )\n",
        "dataserver_test = TextClassification.SentimentAnalysisDataset(\n",
        "                                 train_or_test = 'test',\n",
        "                                 dl_studio = dls,\n",
        " #                            dataset_file = \"sentiment_dataset_test_3.tar.gz\",\n",
        "                               dataset_file = \"sentiment_dataset_test_200.tar.gz\",\n",
        "#                                dataset_file = \"sentiment_dataset_test_40.tar.gz\",\n",
        "                                                                  )\n",
        "text_cl.dataserver_train = dataserver_train\n",
        "text_cl.dataserver_test = dataserver_test\n",
        "\n",
        "text_cl.load_SentimentAnalysisDataset(dataserver_train, dataserver_test)\n",
        "\n",
        "vocab_size = 1 #dataserver_train.get_vocab_size()\n",
        "hidden_size = 512\n",
        "output_size = 2                            # for positive and negative sentiments\n",
        "#,dlsn_layers = 2\n",
        "\n",
        "model = text_cl.TEXTnetOrder2(vocab_size, hidden_size, output_size,dls)\n",
        "\n",
        "number_of_learnable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "num_layers = len(list(model.parameters()))\n",
        "\n",
        "print(\"\\n\\nThe number of layers in the model: %d\" % num_layers)\n",
        "print(\"\\nThe number of learnable parameters in the model: %d\" % number_of_learnable_params)\n",
        "print(\"\\nThe size of the vocabulary (which is also the size of the one-hot vecs for words): %d\\n\\n\" % vocab_size)\n",
        "\n",
        "## TRAINING:\n",
        "print(\"\\nStarting training --- BE VERY PATIENT, PLEASE!  The first report will be at 100th iteration. May take around 5 minutes.\\n\")\n",
        "text_cl.run_code_for_training_for_text_classification_no_gru(model, hidden_size)\n",
        "\n",
        "## TESTING:\n",
        "#import pymsgbox\n",
        "#response = pymsgbox.confirm(\"Finished training.  Start testing on unseen data?\")\n",
        "#if response == \"OK\":\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "The number of layers in the model: 8\n",
            "\n",
            "The number of learnable parameters in the model: 890770\n",
            "\n",
            "The size of the vocabulary (which is also the size of the one-hot vecs for words): 1\n",
            "\n",
            "\n",
            "\n",
            "Starting training --- BE VERY PATIENT, PLEASE!  The first report will be at 100th iteration. May take around 5 minutes.\n",
            "\n",
            "\n",
            "[epoch:1  iter: 100  elapsed_time:   59 secs]     loss: 70.919\n",
            "[epoch:1  iter: 200  elapsed_time:  129 secs]     loss: 46.936\n",
            "[epoch:1  iter: 300  elapsed_time:  184 secs]     loss: 33.881\n",
            "[epoch:1  iter: 400  elapsed_time:  251 secs]     loss: 25.247\n",
            "[epoch:1  iter: 500  elapsed_time:  316 secs]     loss: 23.838\n",
            "[epoch:1  iter: 600  elapsed_time:  373 secs]     loss: 17.526\n",
            "[epoch:1  iter: 700  elapsed_time:  426 secs]     loss: 13.662\n",
            "[epoch:1  iter: 800  elapsed_time:  494 secs]     loss: 8.105\n",
            "[epoch:1  iter: 900  elapsed_time:  549 secs]     loss: 9.786\n",
            "[epoch:1  iter:1000  elapsed_time:  608 secs]     loss: 10.911\n",
            "[epoch:1  iter:1100  elapsed_time:  655 secs]     loss: 6.446\n",
            "[epoch:1  iter:1200  elapsed_time:  716 secs]     loss: 4.066\n",
            "[epoch:1  iter:1300  elapsed_time:  775 secs]     loss: 4.477\n",
            "[epoch:1  iter:1400  elapsed_time:  824 secs]     loss: 3.853\n",
            "[epoch:1  iter:1500  elapsed_time:  887 secs]     loss: 2.839\n",
            "[epoch:1  iter:1600  elapsed_time:  933 secs]     loss: 2.582\n",
            "[epoch:1  iter:1700  elapsed_time:  980 secs]     loss: 2.185\n",
            "[epoch:1  iter:1800  elapsed_time: 1033 secs]     loss: 2.698\n",
            "[epoch:1  iter:1900  elapsed_time: 1091 secs]     loss: 2.158\n",
            "[epoch:1  iter:2000  elapsed_time: 1148 secs]     loss: 1.207\n",
            "[epoch:1  iter:2100  elapsed_time: 1204 secs]     loss: 1.545\n",
            "[epoch:1  iter:2200  elapsed_time: 1260 secs]     loss: 1.523\n",
            "[epoch:1  iter:2300  elapsed_time: 1316 secs]     loss: 1.035\n",
            "[epoch:1  iter:2400  elapsed_time: 1372 secs]     loss: 1.636\n",
            "[epoch:1  iter:2500  elapsed_time: 1419 secs]     loss: 1.225\n",
            "[epoch:1  iter:2600  elapsed_time: 1470 secs]     loss: 1.208\n",
            "[epoch:1  iter:2700  elapsed_time: 1537 secs]     loss: 0.922\n",
            "[epoch:1  iter:2800  elapsed_time: 1595 secs]     loss: 0.892\n",
            "[epoch:1  iter:2900  elapsed_time: 1653 secs]     loss: 0.813\n",
            "[epoch:1  iter:3000  elapsed_time: 1715 secs]     loss: 0.937\n",
            "[epoch:1  iter:3100  elapsed_time: 1772 secs]     loss: 1.094\n",
            "[epoch:1  iter:3200  elapsed_time: 1833 secs]     loss: 0.847\n",
            "[epoch:1  iter:3300  elapsed_time: 1901 secs]     loss: 0.938\n",
            "[epoch:1  iter:3400  elapsed_time: 1955 secs]     loss: 0.905\n",
            "[epoch:1  iter:3500  elapsed_time: 2015 secs]     loss: 0.855\n",
            "[epoch:1  iter:3600  elapsed_time: 2073 secs]     loss: 0.881\n",
            "[epoch:1  iter:3700  elapsed_time: 2122 secs]     loss: 0.729\n",
            "[epoch:1  iter:3800  elapsed_time: 2174 secs]     loss: 0.848\n",
            "[epoch:1  iter:3900  elapsed_time: 2229 secs]     loss: 1.010\n",
            "[epoch:1  iter:4000  elapsed_time: 2281 secs]     loss: 0.797\n",
            "[epoch:1  iter:4100  elapsed_time: 2327 secs]     loss: 0.782\n",
            "[epoch:1  iter:4200  elapsed_time: 2379 secs]     loss: 0.889\n",
            "[epoch:1  iter:4300  elapsed_time: 2429 secs]     loss: 0.869\n",
            "[epoch:1  iter:4400  elapsed_time: 2480 secs]     loss: 0.889\n",
            "[epoch:1  iter:4500  elapsed_time: 2533 secs]     loss: 1.110\n",
            "[epoch:1  iter:4600  elapsed_time: 2584 secs]     loss: 0.778\n",
            "[epoch:1  iter:4700  elapsed_time: 2638 secs]     loss: 0.687\n",
            "[epoch:1  iter:4800  elapsed_time: 2695 secs]     loss: 0.810\n",
            "[epoch:1  iter:4900  elapsed_time: 2754 secs]     loss: 1.013\n",
            "[epoch:1  iter:5000  elapsed_time: 2813 secs]     loss: 1.002\n",
            "[epoch:1  iter:5100  elapsed_time: 2862 secs]     loss: 0.803\n",
            "[epoch:1  iter:5200  elapsed_time: 2926 secs]     loss: 0.717\n",
            "[epoch:1  iter:5300  elapsed_time: 2981 secs]     loss: 0.850\n",
            "[epoch:1  iter:5400  elapsed_time: 3035 secs]     loss: 0.791\n",
            "[epoch:1  iter:5500  elapsed_time: 3081 secs]     loss: 1.150\n",
            "[epoch:1  iter:5600  elapsed_time: 3129 secs]     loss: 0.661\n",
            "[epoch:1  iter:5700  elapsed_time: 3188 secs]     loss: 0.818\n",
            "[epoch:1  iter:5800  elapsed_time: 3255 secs]     loss: 0.889\n",
            "[epoch:1  iter:5900  elapsed_time: 3324 secs]     loss: 0.761\n",
            "[epoch:1  iter:6000  elapsed_time: 3383 secs]     loss: 0.773\n",
            "[epoch:1  iter:6100  elapsed_time: 3436 secs]     loss: 0.696\n",
            "[epoch:1  iter:6200  elapsed_time: 3485 secs]     loss: 0.748\n",
            "[epoch:1  iter:6300  elapsed_time: 3544 secs]     loss: 0.708\n",
            "[epoch:1  iter:6400  elapsed_time: 3614 secs]     loss: 0.742\n",
            "[epoch:1  iter:6500  elapsed_time: 3664 secs]     loss: 0.841\n",
            "[epoch:1  iter:6600  elapsed_time: 3720 secs]     loss: 0.770\n",
            "[epoch:1  iter:6700  elapsed_time: 3785 secs]     loss: 0.833\n",
            "[epoch:1  iter:6800  elapsed_time: 3848 secs]     loss: 0.786\n",
            "[epoch:1  iter:6900  elapsed_time: 3898 secs]     loss: 0.749\n",
            "[epoch:1  iter:7000  elapsed_time: 3950 secs]     loss: 0.779\n",
            "[epoch:1  iter:7100  elapsed_time: 4015 secs]     loss: 0.710\n",
            "[epoch:1  iter:7200  elapsed_time: 4070 secs]     loss: 0.879\n",
            "[epoch:1  iter:7300  elapsed_time: 4117 secs]     loss: 0.741\n",
            "[epoch:1  iter:7400  elapsed_time: 4163 secs]     loss: 0.746\n",
            "\n",
            "Finished Training\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NfWKd5Vj6gnA",
        "outputId": "6c6d58f2-cfd5-4219-c0b2-ee4b9f7bc066",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 399
        }
      },
      "source": [
        "#!/usr/bin/env python\n",
        "\n",
        "##  text_classification_with_gru.py\n",
        "\n",
        "\"\"\"\n",
        "This script is an attempt at solving the sentiment classification problem\n",
        "with an RNN that uses a GRU to get around the problem of vanishing gradients\n",
        "that are common to neural networks with feedback.\n",
        "\"\"\"\n",
        "\n",
        "import random\n",
        "import numpy\n",
        "import torch\n",
        "import os, sys\n",
        "\n",
        "\n",
        "seed = 0\n",
        "random.seed(seed)\n",
        "torch.manual_seed(seed)\n",
        "torch.cuda.manual_seed(seed)\n",
        "numpy.random.seed(seed)\n",
        "torch.backends.cudnn.deterministic=True\n",
        "torch.backends.cudnn.benchmarks=False\n",
        "os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "\n",
        "\n",
        "##  watch -d -n 0.5 nvidia-smi\n",
        "\n",
        "\n",
        "dls = DLStudio(\n",
        "#                  dataroot = \"/home/kak/TextDatasets/\",\n",
        "                  dataroot = '/content/gdrive/My Drive/ECE695/DLStudio/DLStudio-1.1.0/Examples/data/',\n",
        "                  path_saved_model = \"./saved_model\",\n",
        "                  momentum = 0.9,\n",
        "#                  learning_rate =  0.004,\n",
        "                  learning_rate =  1e-4,\n",
        "                  epochs = 1,\n",
        "                  batch_size = 3,\n",
        "                  classes = ('negative','positive'),\n",
        "                  debug_train = 1,\n",
        "                  debug_test = 1,\n",
        "                  use_gpu = True,\n",
        "              )\n",
        "\n",
        "\n",
        "text_cl = TextClassification( dl_studio = dls )\n",
        "dataserver_train = TextClassification.SentimentAnalysisDataset(\n",
        "                                 train_or_test = 'train',\n",
        "                                 dl_studio = dls,\n",
        "  #                              dataset_file = \"sentiment_dataset_train_3.tar.gz\",\n",
        "                               dataset_file = \"sentiment_dataset_train_200.tar.gz\",\n",
        "  #                            dataset_file = \"sentiment_dataset_train_40.tar.gz\",\n",
        "                                                                      )\n",
        "dataserver_test = TextClassification.SentimentAnalysisDataset(\n",
        "                                 train_or_test = 'test',\n",
        "                                 dl_studio = dls,\n",
        "  #                          dataset_file = \"sentiment_dataset_test_3.tar.gz\",\n",
        "                                dataset_file = \"sentiment_dataset_test_200.tar.gz\",\n",
        "  #                           dataset_file = \"sentiment_dataset_test_40.tar.gz\",\n",
        "                                                                  )\n",
        "text_cl.dataserver_train = dataserver_train\n",
        "text_cl.dataserver_test = dataserver_test\n",
        "\n",
        "text_cl.load_SentimentAnalysisDataset(dataserver_train, dataserver_test)\n",
        "\n",
        "vocab_size = 1 #dataserver_train.get_vocab_size()\n",
        "hidden_size = 512\n",
        "output_size = 2                            # for positive and negative sentiments\n",
        "n_layers = 2\n",
        "\n",
        "model = text_cl.GRUnet(vocab_size, hidden_size, output_size, n_layers)\n",
        "\n",
        "number_of_learnable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "num_layers = len(list(model.parameters()))\n",
        "\n",
        "print(\"\\n\\nThe number of layers in the model: %d\" % num_layers)\n",
        "print(\"\\nThe number of learnable parameters in the model: %d\" % number_of_learnable_params)\n",
        "print(\"\\nThe size of the vocabulary (which is also the size of the one-hot vecs for words): %d\\n\\n\" % vocab_size)\n",
        "\n",
        "## TRAINING:\n",
        "print(\"\\nStarting training --- BE VERY PATIENT, PLEASE!  The first report will be at 100th iteration. May take around 5 minutes.\\n\")\n",
        "text_cl.run_code_for_training_for_text_classification_with_gru(model, hidden_size)\n",
        "\n",
        "## TESTING:\n",
        "#import pymsgbox\n",
        "#response = pymsgbox.confirm(\"Finished training.  Start testing on unseen data?\")\n",
        "#if response == \"OK\":\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "The number of layers in the model: 10\n",
            "\n",
            "The number of learnable parameters in the model: 2368002\n",
            "\n",
            "The size of the vocabulary (which is also the size of the one-hot vecs for words): 1\n",
            "\n",
            "\n",
            "\n",
            "Starting training --- BE VERY PATIENT, PLEASE!  The first report will be at 100th iteration. May take around 5 minutes.\n",
            "\n",
            "\n",
            "[epoch:1  iter: 100  elapsed_time:   1 secs]     loss: 0.692\n",
            "[epoch:1  iter: 200  elapsed_time:   3 secs]     loss: 0.699\n",
            "[epoch:1  iter: 300  elapsed_time:   5 secs]     loss: 0.696\n",
            "[epoch:1  iter: 400  elapsed_time:   7 secs]     loss: 0.693\n",
            "[epoch:1  iter: 500  elapsed_time:   8 secs]     loss: 0.695\n",
            "Total Training Time: 27.054418999999143\n",
            "\n",
            "Finished Training\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9MO7mSyfGH7I",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 260
        },
        "outputId": "efe23163-0c56-49c2-a83c-11b4ca9dc0c6"
      },
      "source": [
        "text_cl.run_code_for_testing_text_classification_with_gru(model, hidden_size)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "Number of positive reviews tested: 2937\n",
            "\n",
            "\n",
            "Number of negative reviews tested: 2688\n",
            "\n",
            "\n",
            "Displaying the confusion matrix:\n",
            "\n",
            "                      predicted negative    predicted positive\n",
            "\n",
            "true negative:               3.385            96.615\n",
            "true positive:               3.235            96.765\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0VEQFhD3DKGq"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    }
  ]
}